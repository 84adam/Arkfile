# Test App Plan: Build end-to-end functionality test script: `test-app-curl.sh`

The `test-app-curl.sh` script serves as a comprehensive end-to-end authentication testing framework for the Arkfile secure file vault system. The script validates the complete user lifecycle from initial registration through OPAQUE password-authenticated key exchange, administrative approval workflows, time-based one-time password (TOTP) two-factor authentication setup and verification, session management, and secure cleanup procedures. It exists to ensure that all critical mechanisms function correctly in production environments, providing confidence that users can securely register, authenticate, and maintain sessions within the system. The script currently implements ten distinct testing phases including pre-flight cleanup, OPAQUE registration, administrative API user approval, comprehensive TOTP setup and endpoint validation, login authentication, two-factor authentication verification, basic session management, TOTP management operations, logout procedures, and final comprehensive cleanup.

To extend the script with comprehensive file operations testing, we need to implement additional phases that validate the core file sharing functionality that users expect from the Arkfile system. The proposed file operations testing would require implementing several new testing phases that build upon the existing authenticated session established during the current authentication flow. These new phases would validate that users can perform essential file management operations including listing existing files, encrypting and uploading files, downloading and decrypting files with integrity verification, creating secure sharing links with password protection, and managing the complete sharing lifecycle.

The file listing verification phase would need to test the GET /api/files endpoint with the authenticated user's JWT token to ensure the system correctly returns the user's file inventory in a structured JSON format. This assumes the current system implements this endpoint and that it returns file metadata including file identifiers, names, sizes, upload timestamps, encryption status, sha256 checksums and (potentially also) any sharing information. The API response format should be consistent and include pagination support for users with large file collections.

For the file upload testing, the system would need to handle a 100 MB test file upload through a chunked upload mechanism that supports the Arkfile encryption protocol. This assumes the existence of either a single-request upload endpoint that can handle large files or a chunked upload API that allows files to be transmitted in segments. The upload process must implement client-side encryption before transmission, ensuring that the server receives only encrypted content. The current system architecture suggests that files are encrypted client-side using derived keys, and this encryption must be validated during the upload process. The API would likely be POST /api/files with multipart form data or a specialized chunked upload endpoint that accepts encrypted file segments. If needed, the Go-based command-line tools should be expanded, in order to facilitate the use of the exact same crypto functions as are/will be used in the browser via Go/WASM in order to perform the client-side encryption of files or chunks of files. The Go command-line tools must use exactly the same encryption algorithms and parameters and underlying functions as the Go/WASM functions do.

The download and decryption verification phase requires a corresponding GET /api/files/{fileId}/download endpoint that returns the encrypted file content along with necessary metadata for decryption. The system must provide the authenticated user with their encrypted file content and any required decryption parameters while maintaining the security model where the server never has access to unencrypted file contents. File integrity verification would involve comparing cryptographic hashes of the original file with the downloaded and decrypted content to ensure no corruption occurred during the upload, storage, or download processes. The decryption process must also use the same underlying decryption functions that would be used by the Go/WASM browser client-side functions too.

The sharing functionality implementation requires significant additions to the current system. Creating a sharing link would involve a POST /api/files/{fileId}/share endpoint that generates a unique sharing identifier and accepts a sharing password. The system must implement secure sharing link generation that creates cryptographically strong identifiers while maintaining the encrypted nature of shared files. The sharing mechanism must support password protection where the sharing password is used to derive additional encryption keys, ensuring that even shared files remain encrypted and require the correct sharing password for decryption. The Go command-line tools must be augmented as needed so as to facilitate checking of the password strength to ensure it is sufficiently strong. 

The anonymous sharing access functionality requires implementing public endpoints that do not require user authentication but instead authenticate sharing requests using the sharing link identifier and password combination. This would involve GET /api/shares/{shareId} endpoints that validate sharing credentials and return encrypted file content to anonymous users. The system must implement rate limiting and abuse protection for these public endpoints while maintaining the security model where shared files remain encrypted until the correct sharing password is provided.

The sharing link management capabilities require implementing PUT or DELETE /api/files/{fileId}/share endpoints that allow file owners to modify sharing parameters or disable sharing entirely. This includes the ability to change sharing passwords, set expiration dates, limit the number of downloads, and completely revoke sharing access. The system must maintain sharing audit logs and provide file owners with visibility into sharing activity.

The shared file decryption must also be facilitated by (augmented) Go command-line tools such that the same underlying functions are used both on the command-line as are used in the Go/WASM functions for client-side in-browser decryption.

Current system assumptions include the existence of a robust file storage backend that can handle large encrypted files (MinIO local single node server for testing), implementation of client-side encryption and decryption capabilities that can be replicated in the test environment, proper key derivation mechanisms that support both user authentication and file sharing scenarios, and adequate database schema design to support file metadata, sharing relationships, and access control lists. The system must also implement proper session management that maintains user authentication state throughout extended file operations.

Required enhancements to support comprehensive file operations testing include implementing any missing API endpoints for file management and sharing, ensuring that chunked upload mechanisms can handle large files efficiently, developing client-side encryption and decryption utilities (Go command-line tools) that can be used within the test script environment, implementing comprehensive error handling for file operations including disk space limitations and network interruptions, and creating proper sharing link generation and validation mechanisms that maintain security while enabling anonymous access.

The enhanced test script would need to incorporate file generation utilities that create test files of specified sizes with known content patterns for integrity verification, encryption and decryption functions that replicate the client-side cryptographic operations (Go command-line tools), HTTP request handling that can manage large file uploads and downloads with appropriate timeout settings, and comprehensive verification procedures that validate file integrity through cryptographic hash comparison. The script must also implement proper cleanup procedures that remove test files from both local storage and the server to prevent accumulation of test data in production environments.

Success criteria for the comprehensive file operations testing would include verification that all file API endpoints respond correctly with appropriate HTTP status codes and well-formed JSON responses, confirmation that large file uploads complete successfully with proper encryption and integrity preservation, validation that downloads provide correct encrypted content that decrypts to match original files, demonstration that sharing links function correctly for anonymous access with password protection, proof that sharing link management operations properly control access permissions, and confirmation that all test files and sharing relationships are properly cleaned up after testing completion.

Additionaly future enhancements could include: pausing and resuming of chunked file uploads, initiating multiple uploads simultaneously, and revoking sharing links during download of shared files.

---

<IMPLEMENTATION_OVERVIEW_PHASE_1>

## Phase 1: File Listing and Test File Generation with Encryption

The first phase focuses on validating authenticated file listing capabilities and establishing the foundation for large file testing through generating, encrypting, and preparing a 100 MB test file using the existing OPAQUE-based encryption system. This phase requires creating command-line tools that replicate the exact cryptographic functions used by the Go/WASM client-side implementation to ensure complete compatibility and integrity verification.

The file listing verification functionality can be implemented immediately since the GET /api/files endpoint already exists and returns comprehensive file metadata including filenames, storage IDs, password hints, password types, SHA-256 checksums, file sizes, and upload dates. The test script needs to authenticate using the existing JWT token from the authentication flow, make a request to this endpoint, validate the JSON response structure, and verify that the response includes the expected pagination and storage information fields. The current API returns storage usage statistics including total bytes used, storage limits, available space, and usage percentages, which the test must validate for accuracy.

The 100 MB test file generation requires creating a new Go command-line tool that generates files with known content patterns for integrity verification. The tool must create files with predictable content such as repeated patterns or sequential data that can be easily verified after download and decryption. The generated file must be exactly 100 MB to test the chunked upload system which uses 16 MB chunks, resulting in 7 total chunks with the final chunk being smaller. The tool needs to calculate and store the SHA-256 hash of the original unencrypted content for later integrity comparison.

The encryption functionality must be implemented as a standalone Go command-line tool that replicates the exact OPAQUE-based encryption used by the client/main.go WASM functions. This tool needs to accept an OPAQUE export key, username, file ID, and key type as parameters, then derive the file encryption key using either deriveAccountFileKey or deriveCustomFileKey functions from the crypto package. The encryption process must use AES-GCM with randomly generated nonces and produce output in the exact format expected by the server including proper envelope headers for version and key type information.

For chunked uploads, the command-line tool must implement the encryptFileChunkedOPAQUE functionality where the file is split into 16 MB chunks, each chunk is encrypted separately with its own nonce, and each encrypted chunk includes the nonce, encrypted data, and authentication tag in the format expected by the UploadChunk handler. The tool must calculate SHA-256 hashes for each encrypted chunk and provide these for upload verification. The envelope creation must match the createEnvelopeOPAQUE function using version 0x01 and key type 0x01 for account-based encryption or version 0x02 and key type 0x02 for custom password encryption.

The command-line tools must be designed as extensions to the existing cryptocli framework or as separate utilities that import the same crypto package functions used by the WASM client. The tools need to handle OPAQUE export key input either through command-line parameters, environment variables, or secure file reading. The output must be compatible with the existing API endpoints including proper base64 encoding for data transmission and correct header format for chunk hashes.

Integration with the test script requires modifying test-app-curl.sh to include a new phase_file_operations function that first tests the file listing endpoint, then generates the test file using the command-line tool, encrypts it for chunked upload, initiates an upload session using the existing CreateUploadSession endpoint, uploads all chunks using UploadChunk, and completes the upload using CompleteUpload. The script must validate that the file appears correctly in subsequent file listing requests and that all metadata matches the original file parameters.

Error handling and validation must be comprehensive throughout this phase including verification that all API responses contain expected fields, validation that file sizes match expectations, confirmation that SHA-256 hashes are calculated correctly, and proper cleanup of temporary files and test data. The script must also validate storage quota updates and ensure that user storage statistics are accurately maintained throughout the upload process.

The command-line encryption tool needs to support both account-based and custom password encryption modes to test the full range of encryption capabilities. For account-based encryption, the tool uses the user's OPAQUE export key directly, while custom password encryption requires additional parameters for password derivation. The tool must validate that derived encryption keys are exactly 32 bytes and that all cryptographic operations use the same parameters as the WASM functions including the same AES-GCM configuration and nonce generation methods.

This phase establishes the foundation for all subsequent file operations testing by ensuring that the basic file listing, encryption, and upload mechanisms work correctly with large files and that the command-line tools accurately replicate the client-side cryptographic operations. Success criteria include successful authentication and file listing, generation of a valid 100 MB test file, successful encryption using OPAQUE-derived keys, successful chunked upload of all file parts, verification that the completed file appears in file listings with correct metadata, and confirmation that all cryptographic hashes match expected values throughout the process.

</IMPLEMENTATION_OVERVIEW_PHASE_1>

---

